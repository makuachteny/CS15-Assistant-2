So last time we started on hashing and I just want to do a quick summary, um, which I have here and, uh, um, so we were we were in search for a key value data store. Um, and now we already had some good options, right? Because we have log in binary search trees. And as I pointed out last time, a lot of practical systems offer, um, key value stores based on binary search trees, balanced binary search tree. So those are reasonable options. They're widely used. But there was this nagging desire for for something better. And we were exploring hashing, which holds out the promise of at least in the average case. So for for typical applications and then and most of the time you would get something approaching constant time access. And that seemed really exciting. And uh, uh, so what we want to do is we want to have a constant way to insert, uh, delete and lookup key value pairs in such a store based on the key. Right. So that's, um, that's what we're doing, um, because we want to sort of constant time random access arrays and array lists are sort of the obvious and an obvious underlying data structure, because they provide that, that means of access. Um, and so if we're going to store something in an array or array list based on the index, then we have to get an index on it. And so last time we talked about, well how do we get such an index. If the key is this multi-dimensional struct that has, you know, airline passenger information and it's got military service and it's got Age, and it's about whether they have children and it's got their their so-called fair class, which is something the airlines use to keep track of, you know, how much you paid. And, you know, if you're a frequent flier, then maybe they want you to go, you know, they've got all these these things. And so they're going to form a key. And maybe they just use a confirmation number, but maybe they do something complicated. In any case, in general, if we're going to store data in a key value pair using an array and we want to use indexing, we've got to take arbitrary key data and convert it to an index. And so last time we talked about a two step process for doing it right. Step one was a hash function that takes whatever the data are and converts them whatever the keys are, and converts a key to a non-negative integer. A number okay. So we're going to get and we call that the hash function And we observed. And I informed you last time that hash functions are very easy to do badly and very hard to do well. And so we hire professionals for a lot of this. And C plus plus has a built in hash facility that you that that you can use. And it works pretty well. Um, it's called hash. Uh, you call it like a function, but it's not a function, which is kind of interesting but not relevant today. The point is, they have a they have a facility that they can use that will um, that, that for most for many key types that are common, you can use their hash function and it will do a really good job. Okay. Um, pardon me while I caffeinate myself, but I neglected to caffeinate. Um, okay, so we figured that out. But now you get this number, but it might not be. You know, your table size has, uh, maybe 1023 slots in it or some other number. Who knows? Um, so just getting a number isn't enough to be a valid index into whatever your array is. And we frequently call the array the table because just that's what we think. So the array is the table. Um, and so the hash function takes you from keys to numbers. And then we need a way to go from uh non-negative integers. Uh, and so we need a way to go from there to valid indices. And we call that the compression function, which is a very fancy name for the mod operator basically. Right. Because if you take an integer and you do it mod n, then you'll get a number between zero and n minus one inclusive. And that's what an array index is for an array of size n. Those are the valid indices okay. So um all right. So we have uh we have a search for constant time key value pair operations. We have chosen array lists as an as an underlying representation. Um, we have hash functions that will take keys of whatever type to um, non-negative integers. And then we have the mod operator to take an arbitrary non-negative integer and give you a valid index within an array of some sorts. So we have all that, and we kind of work through some examples. And we uh, we found there was a problem. And the problem was collisions when keys collide with worlds collide. And so a collision occurs when you have two keys that are not the same. But and it really shouldn't be hash there. It should be. Uh, turns out I have that in Emacs. I can just edit it. Okay. Um, so you've got two keys and they hash to the same slot in the array. That's called a collision. And that's an implementors problem because the the client thinks they're different keys. And so they are different keys. They don't compare equal. They're different. And so you need to be able to distinguish them. And the fact that they might go to the same place in your in your storage unit is kind of not the client's problem. That's the implementers problem. So we have to solve this. And it can happen in two ways. One is the hash function itself could produce the same hash for two different keys. Now we hire people to make sure that's pretty rare. And they're good at it. But it's more common. But it's going to happen. And it's seriously going to happen if you, uh. Um, did anybody try running the program? What is it called? Collision detector. Um, it basically just picks random numbers and then, uh, and keeps track of what random numbers you've gotten. I think it mods by a million. And then, um, and then it tells you how many numbers you pick, so you get the same number again. And if you run it, what's interesting is you run it and every time you'll get a different answer. But it's not a big number. You're going to get collisions even even if you have a million slots, you're going to get collisions and you're going to get them pretty soon. Like, you know, within 1000 or 2, it a couple thousand entries is pretty common. So it's going to happen and we have to deal with it. And um, okay. So that's collisions. Everybody up to speed. What hashing is. What are collisions okay. Um, now I often find that folks get confused about collisions. And another problem, which is not collisions, um, which is not a collision. And the problem is sort of a client issue that the clients have to figure out what are the keys in the values. And if the client does a bad job of picking the keys in the values, the implementation can't do anything about that, right? I mean, C plus plus can't stop you from writing an infinite loop. If you want to write an infinite loop, you go right ahead. And it's not, you know, an error in C plus. Plus it's, you know, it's an error in us. We made that mistake, right? Um, so, um, this is in the old days when, uh, when I used to live in Halligan, um, and now I've moved to the JCC, but we're sort of nostalgic for having the to still call the servers the Halligan servers. Have you heard that phrase? Do you know this? The servers for the Halligan servers haven't been in Halligan since, like, 2016, that it moved them out to another facility. Where is it? I don't even know. They they they outsourced it basically to another facility. And the machines used to be sort of in the back of, of Halligan, which is why we call them the Halligan servers, because they were in Halligan. But they haven't been there for many, many years. And yet this is how we we still call them that. But anyway, so I used to live in Halligan, and my colleague Marty used to be in the office next to me. Um, and if you've got this situation where the keys are are dorm names or building names, and the values are the a caffeinated view and the values are, you know, and you think the values are the names of people who live there. Well, it turns out more than one people more want more than one person lives in a dorm. And, um, in this example, Marty and I were there, but Norman was on the other side of my office, and there were, you know, there was all of us were there. Um, and so this is not a collision. This is an application issue where the the application designer has picked a key and there are multiple values that the key should match it, which is not crazy. It's a perfectly reasonable thing, but in that case, the values can't be strings. The values have to be lists of strings, right? The the list of people who live in housing. So that's not a collision. That's an application issue. If you've got because you can see that a collision occurs when the keys are not the same, but the keys are the same, right? So if the keys are the same and the application designer thinks they mapped to different things, they've got a mental block somewhere. And that's not our problem. If we're implementers, we only care for when we get the same slot because we shouldn't have to the same slot. It's the same key, for crying out loud. Of course, it maps to the same slot. We only care if the map is two different keys mapped to the same slot, because that's an implementation aspect. Okay, so I think that's a pretty good summary. Oh not quite because we talked about Cheney. Well, that was interesting after that. Okay, so we've got some exercises. The worst Cheney chaining Cheney challenge. Here we go. So we have to handle them. And last time we talked about Cheney. And today we're going to talk about sort of the other options. So there's sort of two broad categories of solution for for handling collisions. One is Cheney, um, and somebody told me what the strategy for Cheney's.

You read each value of the hash table be a linked list. And then if you had a collision, you just append that item.

Perfect. Um, it could be an array list. It could be any kind of list. But but yes, that's exactly right. What we do is we say each slot in our underlying hash table and our underlying array is not a single value, but is in fact a list. And it's called chaining because in the mental model, a linked list was sort of the idea, but it doesn't have to be. It was a list of structs that have key value pairs. Okay. So we have to we can't not store the key. You might think you don't have to store the key because once you look it up you get the value. But we have to store it because multiple key value pairs might attach to the same slot. And so now you've got multiple things in there in this list. And you have to be able to go through and find. And now you compare the keys with each of the things in the slot to find the right match. Okay. So that was chaining. And I think at this point we're caught up with last time roughly. I gave you a couple things to think about. All right. So The chaining chaining, changing chaining. Where is it? Um. But close. Close. I'm going to go back. Go back, go back. All right. So that brings us to our second approach. Uh, the first approach to handling collisions was chaining. And that one's very easy to understand if multiple things can map to the slot. Put a list of the things that map to the slot in the slot. Easy solution. And you know how to do linear search in an array, and you know how to add in constant time to the front of a linked list or to the back of an array list. Right. So um, um, so we understand chaining, but there are other approaches that fall under the umbrella of what's called open addressing. And there are several variations on this. And we're going to look at three uh, two very briefly we'll look primarily at linear probing. And then we'll look at the other two. Um, but the idea here is that you need an extra space in the table. Um, to, to, um, sort of, uh, as a kind of spoiler. Um, when we analyze these things, what happens if all the slots in your, um, hash table are used up? So in the, in the array, so there's data in all the slots and we're using chaining. Is that okay. It's okay because okay. So gen new things will invariably map to some slot that's already in use. But so what. We already have a solution for that. You just add them to the list in general. You should have rehash by that point right. Well we're going to come to that. So we're going to you don't want the hash table to fill up. And so we're going to talk about that in a minute. So it's an excellent point. What does it mean. We're going to come to that. And so just pause that question. It's an excellent question. But I just right now I just want to address the issue of I just want to put on the table. That chaining works even if you don't do these things, it just works slower. In fact, it devolves to. It devolves to linear right order in the size of the things on the table. Because if you assume that if you've got a decent hash function, they'll be relatively evenly distributed, all the chains should be about the same length, but now it becomes n divided by size of hash table is your is kind of your expected time to look something up. Which is not good. Right. So it's not good but it it doesn't fail. Right. I mean it it produces answer is more slowly open addressing is different. Open addressing cannot succeed. It becomes unusable if the hash table actually fills up. And in a very interesting way okay. So open addressing means we're going to use extra space in the hash table when we get a collision. So that means when you get a collision, it still is going to take up its own slot. There's going to be one key value pair per slot in open addressing. And to handle collisions, then you need an empty space, um, in the hash table. And in fact you're going to need it, um, for other operations too. We'll see how that works out. Okay. So let's look at linear probing, which is um, the foundation of these. And it's the easiest to understand. Um, what we're going to do is if I insert something into a hash table and it goes to a slot, you have to look at the slot and see if there's already something there. Remember, in a computer's memory, there's no such thing as empty memory. So if we. So we're going to have to store boolean values somewhere. You can put a struct in a slot that has, you know, one entry for the key value pair that stored there, and a boolean value that says am I free or not? Am I filled? Am I empty? Right. You pick one. Um, or we can have a parallel array that's the same length as the, uh, as the hash table. That's just an array of Boolean values. True if it's free, false if it's empty or the other way round, you pick and as long as you use them consistent okay. So we're going to need a flag per slot in the array to keep track of whether it's empty or not. Okay. So um, and then what do we do is if there's a collision. So if something hashes boom and it goes to this, oh there's somebody saying okay what do we do. We go to the next slot. Is that free. Yes. Goes there okay. Another thing comes in sadly you get hit again. Oh boom okay. You're full. Oh no you're a full oh no you're a full. Aha. Empty. It goes here. That's linear probe I don't know I need to say much more. I'll show you an example. But that's the idea. Oh the the other thing is all of these things work that when you go off the end of the array, so does it matter that you emerge back at the beginning of the array? Right. So it's plus one mod table size like you saw in the circular buffers, which turns out to be surprisingly useful. Right. Okay. Um, so the table must have extra space for this. You have to wrap around when you reach the end of the array. Um, and you need to know whether we need to record somehow whether a slot is available. So, um, so we'll have an array of these things. There's a that each slot will contain a key value pair, but also a boolean value that says unfilled. Or you can have it in a parallel array. Um, when I say parallel array, do you guys know what that means? Okay. That used to be a standard thing in 11, but I don't know if they talked about it anyway. Um, instead of one alternative to having structs, right? Structs allow you to glue things together. So like a name and an address and an age. Right? Name. Address. Age. Another way to represent that information would be to have three arrays a name array, an age array. And uh, what did I say? Name, age an address and an address array. And then, you know, in slot three of all the arrays, you basically have all of the struct contents. So you can it's another way to handle, um, heterogeneous information as you store all the information in arrays. And they're parallel arrays, because if you know the index. Information related to something at index I in the first array that the information in one of the other arrays at index II is related to the same entity. So it's just another way to if you take databases, you'll see it's another way to sort of stripe data across other data structures. Um, so that's what I mean by parallel array. So instead of one array that has the key value pairs in it, you've got one array that has the key value pairs in it, and another array that's just an array of booleans. And it's the is filled array. Right. So. Not terribly. All right. So let's work in let's work out an example. And we're going to use a terrible hash function as we do because we're trying to study collisions. So unlike a real situation we want collisions because we want to study how to how to handle it. So what we're going to do is we're going to just take the first letter of the word, and we're going to use that as the as the index where A will map to zero, B will map to one in the usual way. We just subtract, take a table. So we insert a 400. So we put that struct that key value pair and the slot for a which is slot zero. Um okay. That goes and swat. See? Okay. Everybody. Right. Aardvark goes in swat a oh, no, oh, no. But of course it can't go an A, but it goes in the next available slot so it goes in B okay, so I think the store problem is actually not too hard, but we'll we'll see. So where it is bag go. Well bag would like to go there but Aardvark is there. So it goes in the next available song. Okay. And drum wants to go indeed. But we just use that for bag. So now it goes there. All right. Um. All right. Well, I told you this was a bad hash function. Okay, so now Carr wants to go in and it's going to go over there. All right, so the algorithm is easy enough. I see many of you are shaking your heads because, uh, the collision seemed to multiply. That's a good thing to be aware of. We'll come back to it. Um, but assuming we've done this, how do we do a lookup now? Right. Because you can't just if you do the lookup, you can't just say, where does Aardvark belong? Oh, it's not there. Sorry, we don't have an aardvark. That would be wrong. We do have an aardvark. It's just not in the first place. He looked because you couldn't store it in the first place. You looked right. So now the search algorithm has to. Has to adapt to that possibility that things may not be in the first place. You look. Um, so what happens is if somebody asks you to look up a key, then what you do is you hash the key, you go to that bucket first. So that's obviously the first place. Um, if it's not there, though, you can't assume you don't have it. You have to keep looking until either you find it or you know it's not there. And one way to know it's not there is if you get to an empty slot, because you know, what the algorithm did was it kept looking for slots. And if it had found an empty slot, it would have put it there. Right. So you get to an empty slot. And remember, whenever I say this, go to the right. It's wrapping around. Right. So the the array is a is a choice. Um okay. So you so this is the album everybody okay. This is going to get worse. You know this is the pattern, right? Uh, what if.

Uh, you like, you put in aardvark and then you remove the pa and then you.

Oh, that's exactly how it's going to get worse. Okay, you've anticipated my next my my next problem. So we're about to answer that I should be able to like, give out free ice cream when people ask questions. It's really good. Yeah. Um, what's the point of this again? Everybody kind of confused about, like, how this what example or what code would we do for us to, like, have to do this? You write a loop that just goes through the array. So, so for for putting something in you say you hash you mod, and then you look at that place in the table. If that table is empty, store. Else you write a loop that just adds one mod table size, and you keep going till you get to an empty space and put it globally. What would make us have to do this if someone assigned you to write a hash table implementation? That could be me or a manager at your company. But okay, so we're taking on, um, the problem of writing the hash table. Um, but it's important that you understand how these things work, even if you're using a hash table, because it has certain implications. Right. So in this case we found aardvark because it was in the next slide. Um, and then if we want to find ball then we, well we hash ball to the B slot. It's not there. But we can't assume because it's not in the first place we looked at isn't there. Right. So we have to keep looking and we go until we get to slot G and find. Oh, okay. That's empty. No ball that. I haven't forgotten you yet. We're coming back. Okay, here we go. Are you ready? Okay, everybody always remove cat. Okay, so we go to see. We keep going until we find cat. In this case, it's right there. And then we remove it from the table and. Okay. So how do you remove it from the table. You don't you can't actually remove stuff from memory. You just set the boolean value to this thing is available now. Right. So you just change the is filled to defaults or the is available to be true whatever you've chosen. Um, but now we have a problem, right? Which you've observed already. If we want to find bag, we go to V. Um, it's not there. We go to see C is empty. And so we say there's not a bag. Oh this is wrong. And you can see in this case that we have that we have that. Huh. All right. So what's the solution?

You have like a calendar of the amount of time we deleted something.

And, um, we could we could have a counter the number of times we deleted something for each.

Number that you like to skip over.

Okay, we can try to keep skip. There's there's a related data structure that I don't want to talk about today but fine idea. And we had two other suggestions I think you were first I think.

Well, the reason this is a problem is because if I deleted an item, it's giving it and noticing an empty space and it's like, okay, well it's not here. Like I tried to pull it in. Would it be possible to kind of like shift everything over?

It's very hard to shift something over without rehashing everything, because you don't know if a thing is in a slot because it hashed there, or because it got there as part of the collision resolution process, which would mean you'd have to rehash it and you sort of have to go through the whole process. So we could do a thing like that and it's going to come up again later. But um, for, for cases like this, we have a simpler solution. I was going to say.

For me to merge with him some more work.

Okay. Um, well. All right. Um, we've already kind of seen a solution, right? Let's think about what state could a slot in the table be in? It could have validated in it. It could be available for stores, or it could be available for stores, but have been occupied before. So three states right? Unfilled. I'm empty. And I've always been empty. I'm empty. But I used to be for. Right. And if you used to be full, then when you're searching, you don't stop. Because the thing that was there might have been, might have been removed, so you just have to keep track of three states. Okay, good.

Couldn't you just like. Because that's going to cause more things to, you know, the table to fill up pretty quickly if you're adding and removing. So could you just like, go, you know, when you remove the cat, you know, just use the maybe some extra time that one time, uh, you know, to just look, see. Okay. We removed, uh, cat. Is there anything else that should go here and then remove that? And you can, like, recursively remove until you.

Get you you can, but you would wind up having to basically rehash order n things. And we're going to come to we're going to come to something about that. Um, so you guys are absolutely right to worry about the table filling up, but we're going to have another solution for that in a minute. Um, okay. So when we remove Cat, we left an unsold slot. Um, but it really wasn't this. It's not the same as other unfilled slots, right? Other unfilled slots that we had seen had never had anything in them. But this slot now is different. You have a question. If you're using the same table for.

Long enough, wouldn't like eventually in the very long term everything become one of those like it used to be filled and now it's unfilled slots, so you'd end up searching the full length of the table. Maybe, maybe.

Maybe. So we're going to we're going to have a solution to that. I keep promising the solution. It turns out it's the same solution for all of these. So that's that's the good news. Uh, these are all perfectly valid problems. Of course. Uh, where was I? Um, yeah. So this just describes the problems that have already been there. Um, okay, so here's how we resolve it. A bucket can be either filled or never has been filled, and is therefore available now, or it's unfilled now. But something was here and it was removed. Uh, all right, so this is. But this is easy. Another parallel array. Or in the first parallel array, you have two bit quantities, or you just add another boolean value. Um, you know, there are there are a variety of ways to program this, but this is one and there are four possible states here. But there are only there are four possible ways to assign true and false to two boolean variables, but only three of them represent a state that can exist in the table based on the algorithms that we have. Um, those three states. Right. So initially filled is false. Removed is false. When you put something in filled becomes true. Remove becomes false. When you remove something filled becomes false, removed becomes true. Those are the three states. And I guess you could have filled in remove. But I would set that.

Back to.

Zero. Um, okay. Okay. So when we remove, um. When we removed that was it, was it bag? It was there.

No, it was cat.

Cat. Thank you. We removed cat. We had to set the remove flag to be true. So now to find bag. Um, we go to the B slot and there's an aardvark. Um, and we keep going until either we find a bag or we hit a non filled non removed bucket and then, uh, right. So not filled and not removed. And so we now we find it. And life is good. Life is good. Um and then we have to tweak our insert algorithm a little bit because um, we move right until we get to the first unfilled bucket and then, um, so that's still so sorry. Um, we can still insert in the first available bucket. Whether something was removed from that slot before or not doesn't matter. Right. It's available. It's not filled. We can put stuff in um, but update find, remove. We have to worry about this until it's everybody. All right. Okay. You guys are surprisingly cool with this. Except you're not cool with the, uh, when the table fills up, which is good, because we're going to worry about that. Um, so a couple things to notice about linear probing. Did you notice that we would get like a cluster and then that would make collisions more likely. And then we'd have to go further and further and further. Um, so this is a problem with linear probe. Now um, you can reduce this problem with a better hash function. If I've got 10,000 slots in my array and my hash function spreads things out, then hopefully it doesn't cluster too much. But the problem is that if you keep putting things right next to each other, it increases. Basically, it increases the size of the bull's eye for the collision. Right. It makes collisions more, more, more likely as time goes by. And of course clusters. In general, if the table is very full, you wind up having to search order n slots in the table. You know, n minus one or something because you keep having to go through these long chains. Um, okay. So linear probing has that problem. Uh, so have a good hash function. That's one part. But another idea is something called quadratic probing. And this idea of sometimes there's a thing called exponential backoff. This is uh, that's related to networks. Um, have you ever had that this experience or are you walking down the hall and there's someone there, and you go this way, and they go this way, and you go this way and this way and this way, and then you do this, and then you do this, and we've all been there. Right. All right. Okay. So how do we resolve that problem? Well, in networks we have the problem where we both try to talk at the same time. And then we pause and then we try to talk again. And if we collide again, then we pause for longer and we try to talk and we collide again. We pause for even longer. Right. So, um, this is a this is a similar sort of idea. The idea is, um, we're going to keep track. We're going to keep attempting to put something in the table. And what we're going to do is we'll go to slot the slot it belongs. And that's attempt zero, right. We'll go to the always start with the thing belongs. That's always the best place. Right. So you start there. Um, and then what happens is if that slot is taken, then um, then you go to the same bucket, but you add the attempt squared. So zero squared and one squared are both zero and one. So the first two places you always try is the slot where belongs and the one immediately to the right of that. But then on attempt two now you're going to say, you know what? Rather than build a cluster, let's go for slots down. Oh that one's empty okay. So let's go nine slots down. And again remember we're wrapping around the you know so there's always a plus nine plus attempt squared mod table size. And we keep going around. Now again the mathematicians are going to realize there's an interesting relationship between the table size and these things. But um and that's true. But I just want to understand the algorithm right now. Do you understand the algorithm. So basically if we start getting a lot of collisions, we'll just we'll make an effort will make a systematic Deterministic effort to spread things out further. So if we're getting a lot of collisions around slot 15, we'll try 15, we'll try 16. But then we're going to try, um, what did I say. Four. So 15 slot 19. And then we'll try 23 and then we'll try um 15 plus 16. Right. So 31 and you, we're going to go further and further okay. Like this. So if the original bucket is five then we're going to try five. We'll add zero squared. Then we'll add one squared. Then we'll add two squared is four five plus four is nine and so on. And but otherwise the algorithms are exactly the same except instead of plus one it's plus a ten squared right. Otherwise the algorithms are exactly the same for insertion removal. Look up everything. We still need to keep the two booleans, right? Because it's still the case that we can remove something and that makes it renders a slot unfilled. But we might have skipped over it before, so we have to keep track. Um. All right. So that's the big idea. How are we doing? Oh, this is great. If I have time today, there's another. I've got two more sorts I want to talk about. How is that? I didn't ask, this was heaps. Right. Did you do a heap sort? Yeah. Yeah. So he talks pretty.

Cool, right?

Okay. Um, I've got another sort for you. So we've got, uh. So now, you know, heap sort. There are two more that I might try to fit in. Well, three more. Um, but anyway, um, if there's time today, there's there's one kind I want to really talk. All right, so the idea here is to avoid those long chains of ever more congested parts of the table. Now remember, most hash tables are not of size 7 or 10. Those are numbers that fit. Those are sizes that fit on the slide. But you know, really they're going to be hundreds or thousands or tens of thousands of slots long. Okay. Well we'll see, we'll see actually. Um, okay. So we talked about linear probing. We talked about quadratic probing where we keep track of the number of attempts. We added ten squared a third approach. All of these are open addressing right. So open addressing use linear probing is open addressing. Quadratic probing is open addressing. And double hashing is another kind of open addressing. Um so double hashing is simple. What you say is well the first hash function produced it a collision. I'll use another hash function. It's independent to the first one, and with high probability it won't collide. It definitely won't give me the same value. Otherwise you don't have two. You don't have independent hash functions. Um. That's it. Now, the problem with that strategy is the second hash function may still produce collisions. And so. Okay. So then we have triple hashing and then quadruple hashing. And then in theory we have a list of hash functions. So you can carry that out I think most people do either 2 or 3. And then and then after that they use some other open addressing thing. They'll use linear probing or quadratic probing. But the hope is that the two hash functions will spread things out. When you look something up, though, you look at the first hash function. You look up the second hash function not there. And then when you get to the last hash function, it devolves to either looking. If you use linear probing, looking until you find an unfilled slot, or um, using the quadratic, uh, probing, you look for an unfilled slot, but you never.

Not have to have a backup thingy like could you like, create, you know, some family of functions that changes based on the order is like the first one. You know where like the number, the try it is, is a variable in that function or something like it's, it's like um.

You so okay, so yes, you can come up with a family of hash functions. The key though is you need a deterministic process that will be the same every time. So that when you look up a key, you have to go through the exact same process you went through when you stored the key. Right. So that's a constraint on you. Basically, you can't like just shuffle the hash functions every time. You can't do that. You can't do a random shuffle. But any kind of deterministic algorithm like that is certainly on the table. Absolutely good if you use this. Was that sorry.

You could if you use the seed.

Uh, so you go through the same sort of pseudo random sequence. Yeah. Yeah. Well, but then it's deterministic. So as long as it's deterministic. Um, okay. So double hashing works well in practice. And did you guys watch the video that I mentioned last time with the WhatsApp things because that that was this, right. They were doing double hashing. And then I don't remember how did they resolve collisions. And did they say, oh no, they're using chaining.

I didn't really understand that they were using a lot of layers and fans.

Uh, okay. So they were using chain. They were using double cat, uh, sorry, double hashing and then chaining. That was what they were doing, because the bug was that someone had a bug where I think they used the same hash function for the first and the second one. So if it collided the first time, of course it collided the second time, because it's the same as. But I think that was the bug. And so then they got like these long chains of 10,000 elements or something.

But if you hash in a hash.

Uh, sure, you can do that. And if you take cryptography, that's just the kind of wild and crazy stuff they do over there. Yeah. So yes, you can do that. Again, the constraint is whatever process you do, you have to be you have to go through both of the lookup and the insertion. Right. So and the delete. So it has to be a deterministic process so that whatever you went through the first time you can you basically you need to be able to if you're looking something up, you have to retrace your steps from when you put it in. So that's the that's the key requirement. But yes, absolutely. You could do that.

The second hash function puts it in a completely different part of the way of the array. And then you look at try to look it up. How will you know that it failed on the first time.

If you if you look the first time and you don't find it, then you always try the second hash function if you're doing double hash. And so basically whatever algorithm you use for putting it in, you use the same algorithm for reconstructing the places to look so absolutely absolute. Um, okay. So we're going to talk about analysis. And then we're going to all those things I kept saying we have a solution. We're going to come to that. So don't worry it's coming. Um, so order n was the worst for chaining because, you know, you assign everything to one slot. Now that slot has n things in it. And so there you go. Um, for open addressing, what do you think the worst case is? I guess I accelerated and loop r um, so. Right. So if the table is actually full, it's an infinite loop. So order infinity. So we're, we're going to say as an invariant of the data structure implementation that the table will never be full. How will guarantee that. Will we'll we'll say so that's good. So assuming it's not infinite what what is it. Well again take the same hash function three. Give me a key three. That's the that's the slot it belongs in. How's that going to work out? Oh, sorry.

Um.

Yeah, it's order in.

Because everything collides. And so in, in the worst case, you have to look at, you have to look through every single element in the table and then find an empty slot to find out that something's not there. Or if the thing you're looking for is the last thing that was stored and they all went to slot one, then it's going to be N slots away from the from the first one is infinity. Is infinity a constant? No, no it's not.

Uh.

Um, it's it's not a, it's not an element to the natural numbers. Um, okay. So for um, you could say, um, order table size, because if you remove things, you still have to search those slots anyway to figure that out. Um, and of course, the more collisions you have, the worse it gets. And yet, not only do they promise this constant time, but they deliver it most of the time, and they're very widely used. It's very easy when you've noticed the worst case, like quicksort. The worst case is not very good. And yet it was used for a long time. And that's because you can sort of mostly keep the worst case from happening. Um, okay. So we're back to this again. Don't use hash functions like the first letter of the word. So I did that once in CS 11. I gave the students a cool project to use. And, um, we I didn't tell them it was a variant of passion. You just tell them what to do when they do it. They don't know any better. And then. And then, uh, um, and it was interesting because we were able to make their code run on average, like 18 to 20 times faster by changing the representation. It was a cool experiment, actually. Um, okay. But then you need an appropriate table size. The. So there's a tradeoff here. Classic time space tradeoff. The bigger the table is, if you have a good hash function, the fewer collisions you're going to have. Duh. Right. Because you do the mod and you're more things that you could get. Right. And if the hash function spreads things out, then it'll sort of hopefully with relatively equal probability, hit each of the available slots there more slots, fewer collisions for the same number of elements. Right. So bigger arrays, fewer collisions. So that's um.

UNKNOWN
That's all right.

You guys know how array this work. And we know that vector is an array list by the way for you should definitely remember how if you're using vectors you need to remember how they work. Because there are some solutions that will fail If you don't understand how vectors work. You might pick a solution where that doesn't matter and then then you're fine. But if you pick a solution where it does matter, you need to know that it matters. So I'll just put that on the table. Um, okay. When we fill up a vector and we need to add a new element, what do we do?

Check if we have an answer.

Check if you have enough space. You don't have enough space. What do you do? You resize. So we're going to do the same thing. We're going to let hash tables grow um until they're until their collision rate goes down. Kind of not quite good. We're not going to measure collisions. We could um, but we measure something else, which is a measure of the probability of a collision. And we call that the load factor. Now the load factor, this computational load factor sort of assumes you have a good hash function that spreads things out pretty uniformly. Um, if that's true, then you can say, well, how full is the table? I guess whether or not the. This definition makes sense no matter what, but it's only really applicable if you realize it's connected to the probability of collisions. So the number of elements in the table I've stored 20,000 keys. And how many slots are in the table 100,000 okay. So that's 20 over 100 I'm 20% full. Right. Makes sense. So we're going to use this. So this thing is called the load factor. And it's basically how full is my table. And the intuition here is if the hash function is good the fullness of the table is basically the probability of a collision. Right. If I've got a, if I, if I, if I've got an empty field and I throw a dart in it, it's not going to hit it. If I have an empty field and I put one person in it and I throw a dart in it, there's a very small chance it will hit that person. If I put 600,000 people on, like, the tough soccer field, I'm going to hit somebody, right? I don't even know if I can do that, but pile on 3D effect. Okay. Um. That's changing. Okay. Um, so if this is a meaningful, uh, notion of probability, then, um, well, first, just do you understand, right. So how many things are in this table? One. Two. Three. Four. Five. Six. So six things in the table. And you see these ellipses here. So there are 26 slots. What do I say. Six things. So six over 26. That's the load factor. Which is a little under a quarter okay.

Um.

If we have open addressing, we can do the same sort of thing, it's easier to count. You don't have to go through this one, two, three, four, five. So we've got five. And this particular table has one, two, three, eight slots. So this is 5/8 full or has a load factor of 5/8 which is that which is 625. Okay. Um if you're using open addressing the load factor must be less than one must. Because if you don't have an empty space, we get to order infinity, right? So, um, chaining you can have a load factor of two. Just, you know, if your hash function is good, then almost then virtually all of the chains will be of length two, a couple will be of length three, a couple will be of length zero, but on average will be two. Um but um for. for open addressing. Um, you have to have a load factor. Strictly less than one, not less than or equal to one. Less than one. Because otherwise the algorithms don't work. Go until you find an empty slot. If there is no empty slot. Go until the universe you know. Go until the Earth falls into the sun or something. That's not that's not going to be helpful. Okay. Um, yeah. So for chaining, um, I've already sort of given this way there's no logical limit. I mean, you don't like it because things get bad as in that video. And thank you for watching because it's fun. And as you go through sort of the program, it's fun to kind of watch these things again, because you'll learn more and more about what they're talking about. Um, there are a couple. There's a really good one about hashing. Sorry, about, uh, hashing that I'm not giving this semester, but, uh, um, when you take 40, it makes a lot more sense. 15 and 40 sort of cover most of what he talks about. Um, okay, so a load factor. A low load factor means low probability of collisions. So we like low, um, load factors. But of course a ton of wasted space has its own implications that are negative. So you want to find some balance. Um, there's not sort of a theoretical that I'm aware of. Any theoretical best value. What I know is in practice, most systems say we'll keep filling the table until the load factor gets 2.7 or 0.75. And at that point we are going to do what we do for ArrayList. We'll just expand the table, we'll double the size. Or again the the the numbers for the sizes are a little bit of an art that I'm not going to talk about in this class, but how did they pick these numbers empirically? They just measured stuff, right? They picked a number and then they measured a suite of applications and timed it, and they picked a different number, and then they time the same suite of applications. They just kind of said, you know, this is the place where we waste a reasonable amount of space and get reasonably good performance. So that's where this that's where these numbers came from. Purely empirical. So we've already talked about it. You know how to do this. But here's the problem. Um, when you expanded a vector you just copy the old values in. Can you do that this time? No you can't. Why not? Yes. You have more spaces for the hashes to go to. Yes. And then you have a hand up your compression. Yes. So the compression function is going to change now. So even the hash function should give you the same value for the things. But the the thing on the right of the mod operator is going to be different. And so it would send the same key to a different slot, even though the hash code is the same. So would you ever have a.

Compression function that's not just the regular model.

I've never seen one. I'm not saying it doesn't exist. I'm saying I've never seen it. Um, but, you know, I'm not a hash table expert, so, um. Okay, so let's look at an example and sort of see how this how this plays out. Um, now this example is not so good, but it's okay. So we suppose this key value pair went to slot nine. And then in this ten slot table. And then we doubled it to 20. Um where should the key go now. What we're going to mod by. So did we say what the. Oh yeah. So the hash was 19. So it did go to slot nine. But now it's going to go to slot 19. And of course it wouldn't still be at the end. It wouldn't necessarily be at the end. Right. That's just an artifact of we picked something nine and ten and 20 had this relationship. But if we had gone from 11 to 253, it could be a very different slot.

So your hash function can be always be able to map to arbitrarily large values that you then compress, because if not, then you will get to a table size which. Oh I see we just always tends to at the first.

Good point. Yeah. So the hash function needs to be able to cover all reasonable hash sizes. The hash table size is a very good point. Um. That's true. I believe that the hash function returns a 64 bit number on our system. So, um, and you can't have an array of size two to the, um, two to the 64, so you're good, but that if you're implementing a small purpose built one, that's the thing that you should consider. Yes, absolutely. Um, okay. Uh, yeah. So the idea is that, um. You make the new hash table and then instead of just copying things over, you use the. You just call your insert function with the new table size insert insert insert, insert. It has to rehash everything. Now that that hash process should take constant time and this process is order in as it was for, so it's still order n. It just adds this fashion compress business. Um, so the analysis is the same as for array lists. Right. And what we saw with array list but did not prove you'll prove in 160 is that in aggregate, in the average case the the amortized cost of the expansion becomes effectively constant time. And you'll, you'll do the recurrence equations in 160. Same analysis applies here. So if you keep the if you keep the load factor within some bound and you grow, eventually you get big enough that it that things stabilize and it kind of works out. So pretty cool. Um, and like I said, every programing language and environment I program in has some form of hashes. Okay. Um, that's a review. Um, what if you just make the table size very large? Well, you can, um, then, um. This is the collision tester dot cp. You can try that out if you think you can make the table large and not implement collisions, you're wrong. That's basically the that's the conclusion. If you run this, you'll you'll see I've got a few minutes. So I think I want to do something else rather than run this. But the the point of this is even if you pick a table size of a million, you're going to get collisions pretty quickly. Um, there's a I think it's on the slide. So yeah. Do you guys know what the birthday paradox. In a group of 23 people, there's a larger than 50% chance two people have the same birthday, despite the fact that there are 365 possible birthdays. Well that's good. I know it's very unintuitive. And this is why I don't do statistics. Um, but for 70 people, it's a 99.9% chance. That's a yeah. So you're not going to win by just making the tables. You should try this. Um. Okay. Um, one last note. I think this is the last. I hope this is the last. I can't remember. Um, keys have to be immutable. I mentioned this before. Keys must be immutable. This is important. Keys must be immutable. Did I say that keys must be immutable? Um, immutable means never changing. So if a key is a string, you can't just go into that string and change the third character to to H. You can't do that. Why? Because once it's in the table, it's in the table based on the hash of the original value. If you change the value and you try to look it up, you'll go somewhere else. And you may never find the first one even though it's there. And then you can't remove it either, because you can't find it necessarily. Right? Not by using the interface. You're not going to find it. You can't assume you'll find it using the interface. You might, but you might not. So um, so that's very important if you're storing if you're storing data, that might change. In particular if you're storing a pointer to something that means something else might be able to get to it using their copy of the pointer. Life can be bad. Just say.

So for rehashing you, you don't need to run it through the hashing function again, right? Because that's the same. It's just the compression function that's well.

So small matter of programing. Um, if you if you don't save the hash, then you have to run it through the hash function. If the hash function is cheap, then then that's not crazy. If the hash function, if that worries you, then instead of storing key value filled you can store key hash. For key you can store it. And then you don't have to run it again. But if you don't want to run it again it's on the implementer to store itself. So it's a good point. Um. Right. So why why would you not use hashes? Because they're awesome, right? And the truth is, people do use them for all kinds of stuff. Well, they're not good if you want to find the min or the max, because that's not how things are stored. Right? So, um, if you want to find min and Max, then, um, well, if you just want to find min and Max using it, if you sometimes want to find min and Max, then you use a balanced binary search. Um, anything that involves linear ordering, though, there's a, um, Python three did something interesting with this, but, uh, um, we'll put that aside for you. Um, but hashing logically has no notion of order, right? And in fact, you don't want the hash function to rely particularly on the order. You want it to spread things out all over the place. And you you want that to be a statistically uniform process. And so you don't want it to be the case that, you know, oh, if I'm, if I'm just if, if I've got a key and then the next key is plus one, it'll be near the first. You don't want that to be true. You want it to be like widely spread out.

Because it will be useful to have some sort of sense of distance between objects that your hash sort of preserves, at least in some degree.

Uh, from the client's point of view, that that's that's invisible. But from an implementation point of view, keeping track of sort of mean space might be useful for as a kind of a additional bit of information to load factor maybe. Could be. Is that where you're going? I don't quite know where you're going.

In terms of like the hashes having no notion of order, where if you had a hash that, say, mapped values that were similar from one side of the thing to similar locations and memory, then if that would not be.

Well, um, so again, I'm not trying to follow 100%, but if you're suggesting that the hash function has a history, then that's problematic, because again, when you insert, you need to look up using the exact same thing. And if the hash function might produce a different value for the same key later, because spacing has worked out, that's not going to work together.

No, that's not what I'm talking about okay.

Yeah. All right. Um, but, uh. Yeah. So I appreciate thinking through these things because it's, uh. Um, well, because it's fun and interesting. Um, so what I'm going to do now is we have just ten minutes. What can I do? I think I can do buckets. So here's what I'm going to do. Um, there are some practice questions and solutions here that you can use for preparing for the final. The second one is very interesting. Um, given an integer list and a target integer, write the pseudocode for a function that returns true if the array contains a pair of elements that sum to the given target false otherwise. Now the naive algorithm for this is n squared, because you said you would sum all pairs of elements and see if you get the target. With a hash table, you can do it in linear time. Um, it is not obvious. And this is questions like this are pretty common in interviews. So that's good interview practice. Um, and you can use it for the final exam, too. Uh. But what I want to do is we have about, uh, 12 minutes left. I don't think I can do all of these, but, um, so I might try to squeeze these in. Um, I want to talk about this because I don't think you should be able to escape from 15 without knowing this information. Escape? Yeah. So we. I will not report final grades until everybody thinks. All right. Um, so we've seen a bunch of sorts. And, um, what we've observed is they go, you know, n squared n squared n squared n log n a, mostly n log n but sometimes n squared. Right. But the best we've seen is n log n. Can you do better. And the groups and the answer is um, is uh if you're comparing elements. No. So if I have to sort things by saying, is this thing less than this one, then I can't do better than n log n. So the interesting thing is, can I sort things without comparing the elements? And the answer is yes. And this is it's just a delay. Um, I think I'll do them in order. Um, so there's sort of there are really three sorts here. There's two, two variations on a thing called bucket sort. And then radix or radix sort is my favorite, but, uh, despite it being my favorite, um, we'll, uh, um, we'll save it maybe for another day. Okay. So, um, if you know that an array just contains zeros, ones and twos or, you know, if you're dealing with hardware, just zeros and ones, do you have to do n log n sorting? No. You can do it in order n time. How are you doing it. Well you know what the values are right. So you take advantage of that. you do an order in pass and you just keep track of the number of zeros, ones and twos, and then you do a linear pass where you write the correct number of zeros, ones and twos in that order. So it's order to n right, which is order n yes. And you don't have to worry about like stability or anything because zero one every zero looks pretty much the same. So. Right. So um knowledge about the domain can often help you write. That's an example of something called uh, um of something called bucket sort. It's also called counting sort. It's a variation of bucket sort where you just you count the number. So I think some people might call it histogram sort. You count the number of things.

Uh, so for this sort of thought, would it like technically not be stable because you're just, you're technically creating new elements instead.

Uh, so, uh, yes, if you're, you're, um, so that this can only be valid if the number, if the things we're using are stuff like integers where the order doesn't matter or.

If you like, know like know everything that's going to be in the thing you thought.

Yes. And that's, that's where it gets, that's, that's where the more general bucket sort of. Okay. So you're exactly right. And um, so this is counting sort and this has um, as you've just observed, it has a very particular domain of application where you're sorting numbers and they're the kinds of things where the order can't matter because and you can produce new ones because I produce a new number. Three. Is that different from the previous three? I had? Not in any way anybody can tell. Right. So it don't in for this particular application. Right. That works very well. Um, so one thing about all bucket swords is you have to know that you have a range of discrete possible values that you're going to sort. And if and if it's more general, then they're the keys, I guess. But you have you have to know that there's a finite range of discrete values. So it can't be like real numbers 3.79263. Right. Integers work really well, but also letters work really well. Right? Because again, you know how many letters there are there. 26 in English. And there's, you know, 127 Ascii code. So you know that there are 127 possibilities. Right? The Huffman tree stuff. You guys are used to that. Um, so the range has to be something that's sensible for your data type, and you have to know what the what the values are. And so then the idea is for each possible value. So you have an array that is range. It's the size of the range right. You have an array that has that many slots in it. And every slot is going to be a bucket. Um, and so like if, you know, things are 0 to 100, like grades work that way. You have a bucket for zero, a bucket for one, bucket for two, bucket for 98 and 99 and 100. You have all these buckets. Um, and usually we'll just have an array just as we've seen before. Um, and then when we're sorting, you just put every element in its bucket. Right. So um, for counting sort, you don't actually have to store the elements. You have to keep track of how many there were. But if you do care, right. And if you care about stability, then what you do is you have to take each thing. So suppose I'm sorting people by their ages. Then I look at their age and I put them in the bucket for their age. And we assume that, you know, let's say ages go from 0 to 200. There's nobody more than 200 years old. So then we just take your age and we put you in the bucket, right? So if you're if you're 19, you go in the 19 bucket. If you're 78, you go in the 78 bucket. And so, um, you can do this without comparing elements, right? Because you just you read the age and you map the index. Now, if the ages go from 15 to 100, then you just you take the age, you subtract 15. And that's the. But it's a very straightforward, efficient computation that critically doesn't depend on the relationship to any of the other values. It's just a computation you can do. Um, so you can you can figure out the bucket a thing belongs in without having to compare it to anything else. Non comparison sort. And you put it there and we've done this. I've done this a lot where you're sort of you're sorting papers. And I often start with a bucket sorting where I have buckets that are, you know everybody a to H everybody I to t or something. Right. And you divide the alphabet up and you put every paper in a bucket. And then you use a different sorting algorithm for the buckets. But the first pass is order in dum dum dum dum dum dum dum because you don't compare the numbers. You don't compare the papers, you just compare. You just look at the first character and it goes in the bucket.

And you're using like floating point numbers or something, and like, you can't make a bucket for every possibility you could. You just make the buckets based on like the part before the decimal and then use another.

You could use you could you could do that. And you can also put them in buckets by range. So you can say anything between, you know, anything from one up to but not including two.

But then they won't be sorted within that.

So you have so you have to do you recursively have to sort the buckets. Um, yeah. So those are that's sort of at the end. So you know, the range of possible values. You create a bucket for each one you put it in. And then to complete the sort, you just read the items out from lowest bucket to highest bucket again without doing any comparisons. This is bucket sorting. It's great time. Okay, so I think we can look through an example. The particular one I told you before was counting sort. Work. Um, and for sorting integers, this works really well. Um, so let's do it. Let's do it. Okay. So let's suppose we have we've done a quiz and they're scored from 0 to 15. Right. Um, okay. So let's, uh, let's sort the quizzes by score in increasing order of scores. So here come the results. Not looking like a very good quiz, but here we go. Um, okay. So six. Well, we've seen a six now plus one. The sixth position. So okay, now we've seen five. Now we've seen nine. Now we've seen a 12. Aha. Now we've seen the nine. Now we've seen a five. So this is why you might call it a histogram sort. Basically we make a histogram. There's a slot for each possible value. And we count how many times it comes up. And then so that's the first pass and that's order n. Well yeah it's order N because it's based on the number of things we passed in. Then the next thing we do is we go. We just go across the array for all possible ranges and you print each non-zero number count times. Okay. So 0000055699 12. Sorted. Um. Okay. Uh oh. I guess the analysis comes later with time to get 2.5 minutes. Um, okay, so that's counting sort. The other thing that Anne was asking about is we can, um, if you've got more complicated data, you're sorting by ID number or by how we sorted, um, by, but. Oh, by grade. So the grade is going to be, um, that number. So we put people in, um, and we store the whole struct. So now a bucket is like a list of structs, right. And then you can still sort them going left to right. And you know everything in a bucket is equal. And so if you want stability you're careful to add them at the back of the list or something like so you have a back pointer and then you can add them back. Or you use a vector and you can add them at the back. Um, and so you can program stability if that's important. Um, but then it's the same idea. You go through the buckets and put things in the result array or print them or whatever you're going to do. So the first step is order n right. Because you put n things in the bucket. And for each thing you do a constant amount of work to examine that thing. You find out the exam score. Maybe you adjust because the the table only goes from 15 to 100, so you subtract 15. But constant amount of work um, end time. So it's n to put things in. Um, to copy things out. It's sort of range plus n because you go through every element of the rain. So that's range. But. The number of things in each bucket will sum to n. Right. So some of the slots will have zero things in them but some will have non-zero things in them. But it doesn't matter what the particular values are, but you know that they have there have to be n things in there, otherwise you've lost some data.

But if you made it, um, another one.

S11
Which I think the last one. I got because you just gave the example, but.

S12
Uh, yeah.

So if you, if you, if you had a way to connect the lists to the non-empty lists, then maybe, but you would. But there's still a, there's still a range size amount of work to do that. But but you're not wrong. You could do that. You could try to merge the lists together that way. Like if you.

S11
Have a .01 to access to, that is automatically connected to the next book.

Right? Right. And you need but you need the front of the next one to be able to. Yes.

S11
Because to find like which bucket to have that.

But that part is what our range size.

S11
Oh, that's what I meant. Sorry.

Yeah. So. So you. But yes, that's absolutely a thing you could do. If you wanted a linked list of a result, then you could. Oh I see. Then you could do order range, size. And then for each one you get the back pointer. So yes you could do that. And then it would devolve to order range size. You're right. You're right. Okay. Now that I've understood what you're saying. Yes, you're absolutely right. Okay, cool. All right. So we did bucket sort, um, and, um. Oh, space. There's extra space. All right. So that's all I want to say about bucket sort. And I'll try to fit in, um, the next one next time. We're out of time. I really like ratings. Great. It's hilarious. So, uh. Uh.

I'm just kidding.

I think.

Yes, Tom. Yeah, it's pretty great. Um. Learning c okay. So learn C and learn to use the app and see things you don't have. So yeah I learned C and particular. Um, and you don't have classes. You don't have instructors. And so you don't have classes you don't have those. But it's like oh yeah.

That's already so we're coming to uh, I don't know. I haven't read it yet. Here. And now. I don't want you to. I guess that makes sense. Yeah, I know.

Bro, I didn't file.

From.

The command line.

All you like.

Because it wasn't came from for us or from me. Yeah.

You're going to get email now.

All right. I'm just, uh, it's just an issue. I mean, I'm on trouble.

No. You'll get an email that says, hey, you shouldn't do that. You. But it happens pretty routinely because, uh, people find stuff on the web, and then they just do it. And the web stuff assumes you own the computer. You're on.

But we don't.

And yeah, right. Staff is not going to let you come superuser and delete anybody else's files. All right. Yeah. So you're not in trouble. But it's going to seem like it because you're going to get an email that says. All right. Cool. Yeah. Professors get a variation of that, but we get the bad professor no biscuit message and students get the you really? You can't do that.

Secondly, I think contains um I it's gone now. I mean oh, you did the collision tester. I tried, but I got a message saying, um, contains isn't I.

Oh, oh, oh, there's a scroll to the top of the file. Must compile with.

That.

Oh, it's a random file then. And I use that instead of but.

Uh, no, no. So when you do you do clang plus plus hyphen std equal sign C plus plus. Okay. Um, no, because I ran it and I got that message and then I put the comment in, so I.

I googled it and it said contained is only in C plus plus.

No that is not what happened. So I did this a few weeks ago. I ran it and it worked. But it was a because it used the old fashioned iterators. And I said come on, there's a contains method. So then I put it and then I go right Okay. And so rather than go back to the ugly version I put in, the thing about, you see, I should put a makefile that would just do that for you. Do you not like it? What's that? You know.

Like it is.

And they're fine. But it just it feels like contains is kind of such a basic thing. It annoys me that it's not there.

I tried.

To do with this raises just now, but I couldn't figure out how to do.

Use the dot begin and end.

Oh, I do like you basically like auto iterator because nobody knows what iterator types are.

Um, and then you want to go back in.

And then you.

Do complicated.

Um while like.

Well iterator does not equal.

And that's kind.

Of.

Because we did that in.

About.

Three months ago. And we.

Say, okay, I'm looking from this point and I want to talk about looking at other data members that are similar to it. So if you're talking about say words as an example. Oh, if you have oh what's a word. That's one letter off.